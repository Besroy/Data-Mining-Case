{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time \n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import random\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP1 ：数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理文件\n",
    "def preprocessing(pathIN,pathOUT,head):\n",
    "    print(\"===========处理开始===========\")\n",
    "    lines=pathIN.readlines()\n",
    "    writerfile=csv.writer(pathOUT)\n",
    "    writerfile.writerow(head)\n",
    "    for line in lines:\n",
    "        writerfile.writerow(line.strip(\"\\n\").split(\"\\x01\"))\n",
    "    print(\"===========处理结束============\")\n",
    "\n",
    "\n",
    "    \n",
    "#计算微博传播规模与传播深度\n",
    "def caculate_width(data):\n",
    "    width_list=data.groupby([\"wbID\"])[\"posted\"].nunique()\n",
    "    return width_list\n",
    "\n",
    "\n",
    "def create_graph(data1,wb_list):\n",
    "    graph_list1={}\n",
    "    #num=1\n",
    "    for i in wb_list:\n",
    "        data=data1[data1[\"wbID\"]==i]\n",
    "        posted_list=list(data.posted.unique())\n",
    "        graph_list2={}\n",
    "        for posted in posted_list:\n",
    "            post=list(data[data[\"posted\"]==posted][\"post\"].unique())\n",
    "            graph_list2[posted]=post\n",
    "        graph_list1[i]=graph_list2\n",
    "        #print(num)\n",
    "       # num+=1\n",
    "    return graph_list1\n",
    "\n",
    "class Vertex:\n",
    "    def __init__(self,num):\n",
    "        self.id=num\n",
    "        self.connectedTo=[]\n",
    "        self.dist=0\n",
    "        \n",
    "    def addNeighbor(self,nbr):\n",
    "        self.connectedTo.append(nbr)\n",
    "        \n",
    "    def setDistance(self,d):\n",
    "        self.dist=d\n",
    "    \n",
    "    def getDistance(self):\n",
    "        return self.dist\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.id)+\" color: \" +self.color+\"dist:\"+str(\n",
    "        self.dist)+\" connectedTo:\"+str([x.id for x in self.connectedTo])\n",
    "    \n",
    "    \n",
    "class Graph:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vertices={}\n",
    "        \n",
    "    def addVertex(self,key):\n",
    "        newVertex=Vertex(key)\n",
    "        self.vertices[key]=newVertex\n",
    "        return newVertex\n",
    "    \n",
    "    def getVertex(self,n):\n",
    "        vertex=self.vertices.get(n)\n",
    "        return vertex\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.vertices.values())\n",
    "    \n",
    "def bfs(start):\n",
    "    start.setDistance(1)\n",
    "    distance=1\n",
    "    vertlist=[]\n",
    "    vertlist.append(start)\n",
    "    \n",
    "    while (len(vertlist)>0):\n",
    "        currentVert=vertlist.pop()\n",
    "        for nbr in currentVert.connectedTo:\n",
    "            nbr.setDistance(currentVert.getDistance()+1)\n",
    "            if nbr not in vertlist:\n",
    "                vertlist.insert(0,nbr)\n",
    "        if distance <currentVert.getDistance():\n",
    "            distance=currentVert.getDistance()\n",
    "    return distance\n",
    "\n",
    "def caculate_depth(a,graph_list1,wb_list):\n",
    "    depth_list=[]\n",
    "    #num=1\n",
    "    for i in wb_list:\n",
    "        posted=a[a[\"wbID\"]==i][\"posted\"].values[0]\n",
    "        graph=Graph()\n",
    "       # print(num)\n",
    "       # num+=1\n",
    "        if graph_list1[i]=={}:\n",
    "            depth_list.append(0)\n",
    "            continue  \n",
    "        \n",
    "        if posted not in list(graph_list1[i].keys()):\n",
    "            depth_list.append(1)\n",
    "        else:\n",
    "            for key,value in graph_list1[i].items():\n",
    "                vertex=graph.addVertex(key)\n",
    "            key_list=[]\n",
    "            for key,values in graph_list1[i].items():\n",
    "                vertex=graph.getVertex(key)\n",
    "                key_list.append(key)\n",
    "                for value in values:\n",
    "                    ner_vertex=graph.getVertex(value)\n",
    "                    if (ner_vertex != None)&(value not in key_list):\n",
    "                        vertex.addNeighbor(ner_vertex)\n",
    "            depth_list.append(bfs(graph.vertices[posted]))\n",
    "\n",
    "    return depth_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP2 ：特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建用户粉丝表\n",
    "azip = zipfile.ZipFile('./data/userRelations.zip')\n",
    "f1 = azip.open('weibo_dc_parse2015_link_filter')\n",
    "path = pd.read_csv(f1, nrows=500000)\n",
    "relation_list={}\n",
    "for line in path:\n",
    "    fan=line.strip(\"\\n\").split(\"\\t\")[0]\n",
    "    star=line.strip(\"\\n\").split(\"\\t\")[1].split(\"\\x01\")\n",
    "    for i in star:\n",
    "        if i not in relation_list.keys():\n",
    "            relation_list[i]=[]\n",
    "            relation_list[i].append(fan)\n",
    "        else:\n",
    "            relation_list[i].append(fan)\n",
    "\n",
    "stars=list(relation_list.keys())\n",
    "fans_number=[]\n",
    "for star in stars:\n",
    "    fans_number.append(len(relation_list[star]))\n",
    "fans_number_list=pd.DataFrame({\"user\":stars,\"fans_number\":fans_number})\n",
    "\n",
    "#print(type(fans_number_list))\n",
    "\n",
    "fans_number_list.to_csv('./data/fans_number_list.csv')\n",
    "fans_number_list = pd.read_csv('./data/fans_number_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fp=open(\"context.txt\",\"r\",encoding=\"utf-8\")\n",
    "def Features1(context,fp,fans_number_list):              \n",
    "    #加入各个用户的粉丝数量作为特征\n",
    "    features=context[[\"wbID\",\"posted\"]]\n",
    "    features=features.merge(fans_number_list,left_on=\"posted\",right_on=\"user\",how=\"left\")\n",
    "    \n",
    "    #time\n",
    "    features[\"hour\"]=context.time.apply(lambda x:int(x.split(\":\")[0]))\n",
    "    \n",
    "    #发布时间距离零点的分钟数\n",
    "    time=pd.DataFrame({\"wbID\":context[\"wbID\"],\"time\":pd.to_datetime(context[\"time\"])})\n",
    "    a=pd.to_datetime(\"2019-05-27 00:00:00\")  #时间根据运行当天的日期改一下\n",
    "    b=pd.to_datetime(\"2019-05-28 00:00:00\")\n",
    "    distance_0=[]\n",
    "    for i in list(time[\"time\"]):\n",
    "        distance_0.append(min(abs((i-a).seconds/60),abs((b-i).seconds/60)))\n",
    "    time[\"distance_0\"]=distance_0\n",
    "    features=features.merge(time[[\"wbID\",\"distance_0\"]],on=\"wbID\",how=\"left\")\n",
    "    \n",
    "    #源微博文本中包含一些特殊符号或链接的数量\n",
    "    topic_num=[]\n",
    "    link_num=[]\n",
    "    name_num=[]\n",
    "    book_num=[]\n",
    "    emoji_num=[]\n",
    "    at_num=[]\n",
    "\n",
    "    topic=re.compile(\"#([^@]+?)#\")\n",
    "    link=re.compile(\"([hH][tT][tT][pP][sS]?:\\\\/\\\\/[^ ,‘\\\">\\\\]\\\\)]*[^\\\\. ,‘\\\">\\\\]\\\\)])\")\n",
    "    emoji=re.compile(\"\\\\[([^@]+?)\\\\]\")\n",
    "    book=re.compile(\"《([^@]+?)》\")\n",
    "    at=re.compile(\"@[0-9a-zA-Z\\\\u4e00-\\\\u9fa5]+\")\n",
    "    name=re.compile(\"【([^@]+?)】\")\n",
    "    for line in fp.readlines():\n",
    "        topic_num.append(len(topic.findall(line)))\n",
    "        link_num.append(len(link.findall(line)))\n",
    "        emoji_num.append(len(emoji.findall(line)))\n",
    "        name_num.append(len(name.findall(line)))\n",
    "        book_num.append(len(book.findall(line)))\n",
    "        at_num.append(len(at.findall(line)))\n",
    "    features[\"topic_num\"]=topic_num\n",
    "    features[\"link_num\"]=link_num\n",
    "    features[\"emoji_num\"]=emoji_num\n",
    "    features[\"name_num\"]=name_num\n",
    "    features[\"book_num\"]=book_num\n",
    "    features[\"at_num\"]=at_num\n",
    "    return features\n",
    "\n",
    "def features2(repost_new,fans_number_list,features):\n",
    "    #统计15、30、45、60分钟这几个时刻的发布者粉丝数量\n",
    "    for time in [15,30,45,60]:\n",
    "        data=repost_new[repost_new[\"time\"]<=time*60]  \n",
    "        data = data.merge(fans_number_list,left_on='post',right_on='user')\n",
    "        fans_sum=pd.DataFrame(data.groupby([\"wbID\"])[\"fans_number\"].sum()).reset_index()\n",
    "        fans_mean=pd.DataFrame(data.groupby([\"wbID\"])[\"fans_number\"].mean()).reset_index()\n",
    "        fans_max=pd.DataFrame(data.groupby([\"wbID\"])[\"fans_number\"].max()).reset_index()\n",
    "        fans_sum.columns=[\"wbID\",\"fans_sum_%d\"%(time)] \n",
    "        fans_mean.columns=[\"wbID\",\"fans_mean_%d\"%(time)]\n",
    "        fans_max.columns=[\"wbID\",\"fans_max_%d\"%(time)]\n",
    "        features=features.merge(fans_sum,on=\"wbID\",how=\"left\").merge(fans_mean,on=\"wbID\",how=\"left\").merge(fans_max,on=\"wbID\",how=\"left\")\n",
    "    \n",
    "    \n",
    "    #print(features)\n",
    "    #统计截至一个小时源微博发布者与转发者粉丝之间的差异\n",
    "    features[\"fans_mean_diff\"]=features[\"fans_number\"]-features[\"fans_mean_60_y\"]\n",
    "    features[\"fans_max_diff\"]=features[\"fans_number\"]-features[\"fans_max_60\"]\n",
    "    features[\"fans_sum_diff\"]=features[\"fans_number\"]-features[\"fans_sum_60\"]\n",
    "    return features\n",
    "\n",
    "def features3(wb_list,repost_new,a,features):\n",
    "    depth=pd.DataFrame({\"wbID\":wb_list})\n",
    "    width=pd.DataFrame({\"wbID\":wb_list})\n",
    "    for i in range(15,75,15):\n",
    "        data=repost_new[repost_new[\"time\"]<=i*60]\n",
    "        graph_list1=create_graph(data,wb_list)\n",
    "        depth_list1=caculate_depth(a,graph_list1,wb_list)\n",
    "        depth[\"depth_%d_min\"%(i)]=depth_list1\n",
    "        width_list1=caculate_width(a)\n",
    "        width=width.merge(width_list1,on=\"wbID\",how=\"left\")\n",
    "    width.columns=[\"wbID\",\"width_15_min\",\"width_30_min\",\"width_45_min\",\"width_60_min\"]\n",
    "    features=features.merge(depth,on=\"wbID\",how=\"left\").merge(width,on=\"wbID\",how=\"left\")\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP3 :模型优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelOptimization(model, params, x_train, y_train):\n",
    "    \"\"\"\n",
    "        模型优化：用梯度搜索方法调参，找到模型的最优参数\n",
    "        参数：model代表模型，本案例中使用XGBoost\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Model Optimizatioin Start\")\n",
    "    x_train = x_train.fillna(-1)\n",
    "    best_params = []\n",
    "    for param in params:\n",
    "        print(\"Optimize param\", param, \"...\")\n",
    "        cv = GridSearchCV(estimator = model, param_grid = param, scoring = \"r2\", cv = 3, n_jobs = -1)\n",
    "        cv.fit(x_train, y_train)\n",
    "        best_params.append(cv.best_params_)\n",
    "    print(\"Model Optimizatioin Done\")\n",
    "    return best_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP4 :模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelEvaluation(model_width,model_depth, x_train, y_train):\n",
    "    \"\"\"\n",
    "        模型评估：评估模型效果，\n",
    "        参数：model_width代表微博规模预测模型，model_depth代表微博深度预测模型，x_train、y_train分别代表训练集特征和含标签的数据集\n",
    "        注意：在评估lgb时，将该函数加入一个parameters参数\n",
    "    \"\"\"\n",
    "    print(\"Model Evaluation Start\")\n",
    "    weiboID=list(x_train[\"wbID\"].unique())\n",
    "    testID=random.sample(weiboID,int(len(weiboID)*0.3))\n",
    "    trainID=[i for i in weiboID if i not in testID ]\n",
    "    \n",
    "    # 计算MPAE\n",
    "    print(\"Compute MAPE_width...\")\n",
    "    x_tr, x_te = x_train[x_train[\"wbID\"].isin(trainID)],x_train[x_train[\"wbID\"].isin(testID)]\n",
    "    y_tr, y_te = y_train[y_train[\"wbID\"].isin(trainID)],y_train[y_train[\"wbID\"].isin(testID)]\n",
    "    #dtrain=model_width.Dataset(data=x_tr.drop(\"wbID\",axis=1),label=y_tr[\"y_width\"])  #当评估lgb时使用这一段代码\n",
    "    #dvalid=model_width.Dataset(data=x_te.drop(\"wbID\",axis=1),label=y_te[\"y_width\"])  #当评估lgb时使用这一段代码\n",
    "    #clf=model_width.train(parameters,dtrain,num_boost_round=2000,verbose_eval=50)    #当评估lgb时使用这一段代码\n",
    "    #y_pred_width = clf.predict(x_te.drop(\"wbID\",axis=1))                           #当评估lgb时使用这一段代码\n",
    "    model_width.fit(x_tr.drop(\"wbID\",axis=1),y_tr[\"y_width\"])           #当评估lgb时不使用这段代码\n",
    "    y_pred_width = model_width.predict(x_te.drop(\"wbID\",axis=1))      #当评估lgb时不使用这段代码\n",
    "    y_te[\"y_pred_width\"]=y_pred_width\n",
    "    y_te[\"mae_width\"]=abs(y_te[\"y_pred_width\"]-y_te[\"y_width\"])/y_te[\"y_pred_width\"]\n",
    "    mape_width=np.mean(y_te.groupby([\"time\"])[\"mae_width\"].mean())\n",
    "    \n",
    "    print(\"Compute MAPE_depth...\")\n",
    "    model_depth.fit(x_tr.drop(\"wbID\",axis=1), y_tr[\"y_depth\"])       #当评估lgb时不使用这段代码\n",
    "    y_pred_depth = model_depth.predict(x_te.drop(\"wbID\",axis=1))      #当评估lgb时不使用这段代码\n",
    "    #clf2=model_depth.train(parameters,dtrain,num_boost_round=2000,verbose_eval=50)  #当评估lgb时使用这段代码\n",
    "    #y_pred_depth = clf2.predict(x_te.drop(\"wbID\",axis=1))                           #当评估lgb时使用这段代码\n",
    "    y_te[\"y_pred_depth\"]=y_pred_depth\n",
    "    y_te[\"mae_depth\"]=abs(y_te[\"y_pred_depth\"]-y_te[\"y_depth\"])/y_te[\"y_pred_depth\"]\n",
    "    mape_depth=np.mean(y_te.groupby([\"time\"])[\"mae_depth\"].mean())\n",
    "    \n",
    "    mape=0.7*mape_width+0.3*mape_depth\n",
    "    print(\"Model Evaluation Done\")\n",
    "    return mape,y_pred_width,y_te[\"y_width\"],y_pred_depth,y_te[\"y_depth\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========处理开始===========\n"
     ]
    }
   ],
   "source": [
    "#将trainRepost.txt转化为数据框格式\n",
    "pathIN=open(\"./data/trainRepost.txt\",\"r\",encoding=\"utf-8\")\n",
    "pathOUT=open(\"./data/trainRepost_new.csv\",\"w\",encoding=\"utf-8\")\n",
    "head=[\"wbID\",\"posted\",\"post\",\"time\",\"content\"]\n",
    "preprocessing(pathIN,pathOUT,head)\n",
    "\n",
    "#将weibofrofile.train转化为数据框格式\n",
    "pathIN_1=open(\"./data/WeiboProfile.train\",\"r\",encoding=\"utf-8\")\n",
    "pathOUT_1=open(\"./data/trainProfile_new.csv\",\"w\",encoding=\"utf-8\")\n",
    "head_1=[\"wbID\",\"posted\",\"time\",\"context\"]\n",
    "preprocessing(pathIN_1,pathOUT_1,head_1)\n",
    "\n",
    "#将内容一行一行的提取出来形成txt文档\n",
    "csv_data = pd.read_csv('./data/trainProfile_new.csv')\n",
    "\n",
    "column = csv_data['context']\n",
    "f = open('./data/context.txt', 'w',encoding='utf-8')\n",
    "\n",
    "for i in column:\n",
    "    f.writelines(i)\n",
    "    f.writelines('\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已经处理的时刻： 75\n"
     ]
    }
   ],
   "source": [
    "#read data\n",
    "path=open(\"./data/trainRepost_new.csv\",\"r\",encoding=\"utf-8\")\n",
    "repost=pd.read_csv(path, nrows=500000)\n",
    "repost_new=repost.sort_values([\"wbID\",\"time\"],ascending=True)\n",
    "a=pd.read_csv(\"./data/trainProfile_new.csv\",encoding=\"utf-8\")\n",
    "wb_list=list(set(list(repost_new.wbID.unique())).intersection(set(list(a.wbID.unique()))))\n",
    "\n",
    "\n",
    "#calulate depth & width\n",
    "label_depth=pd.DataFrame({\"wbID\":wb_list})\n",
    "label_width=pd.DataFrame({\"wbID\":wb_list})\n",
    "\n",
    "#缩减时间\n",
    "for i in range(75, 90, 15):\n",
    "#for i in range(75,4395,15):\n",
    "    data=repost_new[repost_new[\"time\"]<=i*60]\n",
    "    graph_list1=create_graph(data,wb_list)\n",
    "    depth_list1=caculate_depth(a,graph_list1,wb_list) #caculate_depth函数详见上述代码\n",
    "    width_list1=caculate_width(data)#caculate_width函数详见上述代码\n",
    "    label_depth[i]=depth_list1\n",
    "    label_width=label_width.merge(width_list1,on=\"wbID\",how=\"left\")\n",
    "    print(\"已经处理的时刻：\",i)\n",
    "\n",
    "# write data\n",
    "label_depth.to_csv(\"./data/label_depth.csv\",\"r\",encoding=\"utf-8\")\n",
    "label_width.to_csv(\"./data/label_width.csv\",\"r\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程&模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation Start\n",
      "Compute MAPE_width...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute MAPE_depth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation Done\n",
      "1.2384863905701613\n",
      "Model Evaluation Start\n",
      "Compute MAPE_width...\n",
      "[15:56:48] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/usr/local/lib/python3.6/dist-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute MAPE_depth...\n",
      "[15:56:50] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Model Evaluation Done\n",
      "0.8135881583395294\n"
     ]
    }
   ],
   "source": [
    "label_depth=label_depth.set_index(\"wbID\").stack().reset_index()\n",
    "label_width=label_width.set_index(\"wbID\").stack().reset_index()\n",
    "label_depth.columns=[\"wbID\",\"time\",\"y_depth\"]\n",
    "label_width.columns=[\"wbID\",\"time\",\"y_width\"]\n",
    "\n",
    "#print(label_depth[0:100000])\n",
    "#print(label_width[0:100000])\n",
    "\n",
    "#label_depth.to_csv('./data/1.csv')\n",
    "#label_depth = pd.read_csv('./data/1.csv')\n",
    "\n",
    "#label_width.to_csv('./data/2.csv')\n",
    "#label_width = pd.read_csv('./data/2.csv')\n",
    "\n",
    "label_depth['wbID'] = pd.to_numeric(label_depth['wbID'], errors='coerce')\n",
    "label_depth['time'] = pd.to_numeric(label_depth['time'], errors='coerce')\n",
    "label_width['wbID'] = pd.to_numeric(label_width['wbID'], errors='coerce')\n",
    "label_width['time'] = pd.to_numeric(label_width['time'], errors='coerce')\n",
    "\n",
    "#print(type(label_depth))\n",
    "#print(label_depth['wbID'])\n",
    "\n",
    "\n",
    "y_train=pd.merge(label_depth,label_width,on=[\"wbID\",\"time\"],how=\"outer\")\n",
    "\n",
    "context=pd.read_csv(\"./data/trainProfile_new.csv\",encoding=\"utf-8\")\n",
    "fp=open(\"./data/context.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "features=Features1(context,fp,fans_number_list)\n",
    "\n",
    "#features.to_csv('./data/2.csv')\n",
    "#features = pd.read_csv('./data/2.csv')\n",
    "\n",
    "train_data=y_train.merge(features,on=\"wbID\",how=\"left\")\n",
    "train_data=train_data[train_data[\"y_depth\"]!=0]\n",
    "\n",
    "x_train1=train_data[['wbID','hour', 'fans_number',  'distance_0', 'topic_num', 'link_num','name_num',  'emoji_num', \"time\"]]\n",
    "#print(x_train1)\n",
    "#x_train1=train_data[['wbID','hour', 'fans_number',  'distance_0', 'topic_num', 'link_num','name_num', 'other_num', 'emoji_num', 'other1_num',\"time\"]]\n",
    "y_train1=train_data[['wbID',\"y_width\",\"y_depth\",\"time\"]]\n",
    "x_train1[\"fans_number\"]=x_train1[\"fans_number\"].fillna(x_train1[\"fans_number\"].mean())\n",
    "\n",
    "x_train1[\"time\"]=pd.to_numeric(x_train1[\"time\"])\n",
    "x_train1 = x_train1.fillna(0)\n",
    "y_train1 = y_train1.fillna(1)\n",
    "\n",
    "rfr1= ensemble.RandomForestRegressor()\n",
    "rfr2= ensemble.RandomForestRegressor()\n",
    "\n",
    "\n",
    "mape_rf1,y_pred_width_rf1,y_width_rf1,y_pred_depth_rf1,y_depth_rf1=ModelEvaluation(rfr1,rfr2,x_train1, y_train1)\n",
    "print(mape_rf1)\n",
    "\n",
    "xgb1=XGBRegressor()\n",
    "xgb2=XGBRegressor()\n",
    "mape_xgb1,y_pred_width_xgb1,y_width1,y_pred_depth_xgb1,y_depth1=ModelEvaluation(xgb1,xgb2,x_train1, y_train1)\n",
    "print(mape_xgb1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparameters = {\\n    \\'application\\': \\'regression\\',\\n    \\'objective\\': \"regression\",\\n    \\'metric\\': \\'mse\\',\\n    \\'is_unbalance\\': \\'true\\',\\n    \\'boosting\\': \\'gbdt\\',\\n    \\'num_leaves\\': 31,\\n    \\'feature_fraction\\': 0.5,\\n    \\'bagging_fraction\\': 0.5,\\n    \\'bagging_freq\\': 20,\\n    \\'learning_rate\\': 0.05,\\n    \\'verbose\\': 0\\n}\\nmodel_width=lgb\\nmodel_depth=lgb\\n\\nmape_lgb,y_pred_width_lgb,y_te_width_lgb,y_pred_depth_lgb,y_te_depth_lgb=ModelEvaluation(model_width,model_depth, x_train1, y_train1,parameters)\\nprint(\"score:\",mape_lgb)\\n\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "parameters = {\n",
    "    'application': 'regression',\n",
    "    'objective': \"regression\",\n",
    "    'metric': 'mse',\n",
    "    'is_unbalance': 'true',\n",
    "    'boosting': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.5,\n",
    "    'bagging_freq': 20,\n",
    "    'learning_rate': 0.05,\n",
    "    'verbose': 0\n",
    "}\n",
    "model_width=lgb\n",
    "model_depth=lgb\n",
    "\n",
    "mape_lgb,y_pred_width_lgb,y_te_width_lgb,y_pred_depth_lgb,y_te_depth_lgb=ModelEvaluation(model_width,model_depth, x_train1, y_train1,parameters)\n",
    "print(\"score:\",mape_lgb)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3391: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[k1] = value[k2]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation Start\n",
      "Compute MAPE_width...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute MAPE_depth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation Done\n",
      "1.2319708161220575\n",
      "Model Evaluation Start\n",
      "Compute MAPE_width...\n",
      "[13:42:40] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/usr/local/lib/python3.6/dist-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute MAPE_depth...\n",
      "[13:42:43] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Model Evaluation Done\n",
      "0.8190815210092893\n"
     ]
    }
   ],
   "source": [
    "repost_new=pd.read_csv(\"./data/trainRepost_new.csv\", nrows=500000,encoding=\"utf-8\")\n",
    "\n",
    "#print(repost_new)\n",
    "#print(fans_number_list)\n",
    "\n",
    "features=features2(repost_new,fans_number_list,features)\n",
    "train_data=y_train.merge(features,on=\"wbID\",how=\"left\")\n",
    "train_data=train_data[train_data[\"y_depth\"]!=0]\n",
    "\n",
    "#print(train_data)\n",
    "\n",
    "\n",
    "x_train2=train_data[['wbID','hour', 'fans_number', 'fans_sum_30',\n",
    "       'fans_mean_30', 'fans_max_30', 'fans_sum_15', 'fans_mean_15',\n",
    "      'fans_max_15', 'fans_sum_45', 'fans_mean_45', 'fans_max_45',\n",
    "       'fans_sum_60', 'fans_mean_60', 'fans_max_60', 'fans_mean_diff',\n",
    "       'fans_max_diff', 'fans_sum_diff', 'distance_0', 'topic_num', 'link_num',\n",
    "       'name_num', 'emoji_num',\"time\"]]\n",
    "\n",
    "y_train2=train_data[['wbID',\"y_width\",\"y_depth\",\"time\"]]\n",
    "x_train2[['fans_sum_30', 'fans_mean_30', 'fans_max_30', 'fans_sum_15', 'fans_mean_15',\n",
    "      'fans_max_15', 'fans_sum_45', 'fans_mean_45', 'fans_max_45',\n",
    "       'fans_sum_60', 'fans_mean_60', 'fans_max_60', 'fans_mean_diff',\n",
    "       'fans_max_diff', 'fans_sum_diff',\"fans_number\"]]=x_train2[['fans_sum_30', 'fans_mean_30', 'fans_max_30', 'fans_sum_15', 'fans_mean_15',\n",
    "      'fans_max_15', 'fans_sum_45', 'fans_mean_45', 'fans_max_45',\n",
    "       'fans_sum_60', 'fans_mean_60', 'fans_max_60', 'fans_mean_diff',\n",
    "       'fans_max_diff', 'fans_sum_diff',\"fans_number\"]].fillna(x_train2[['fans_sum_30', 'fans_mean_30', 'fans_max_30', 'fans_sum_15', 'fans_mean_15',\n",
    "      'fans_max_15', 'fans_sum_45', 'fans_mean_45', 'fans_max_45',\n",
    "       'fans_sum_60', 'fans_mean_60', 'fans_max_60', 'fans_mean_diff',\n",
    "       'fans_max_diff', 'fans_sum_diff',\"fans_number\"]].mean())\n",
    "x_train2[\"time\"]=pd.to_numeric(x_train2[\"time\"])\n",
    "x_train2 = x_train2.fillna(0)\n",
    "y_train2 = y_train2.fillna(1)\n",
    "\n",
    "rfr1= ensemble.RandomForestRegressor()\n",
    "rfr2= ensemble.RandomForestRegressor()\n",
    "mape_rf2,y_pred_width_rf2,y_width_rf2,y_pred_depth_rf2,y_depth_rf2=ModelEvaluation(rfr1,rfr2,x_train2, y_train2)\n",
    "print(mape_rf2)\n",
    "\n",
    "xgb1=XGBRegressor()\n",
    "xgb2=XGBRegressor()\n",
    "mape_xgb2,y_pred_width_xgb2,y_width2,y_pred_depth_xgb2,y_depth2=ModelEvaluation(xgb1,xgb2,x_train2, y_train2)\n",
    "print(mape_xgb2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "mape_lgb2,y_pred_width_lgb2,y_te_width_lgb2,y_pred_depth_lgb2,y_te_depth_lgb2=ModelEvaluation(model_width,model_depth, x_train2, y_train2,parameters)\n",
    "print(\"score:\",mape_lgb2)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3391: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[k1] = value[k2]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation Start\n",
      "Compute MAPE_width...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute MAPE_depth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation Done\n",
      "2.8645107905374595e-05\n",
      "Model Evaluation Start\n",
      "Compute MAPE_width...\n",
      "[18:05:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/usr/local/lib/python3.6/dist-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute MAPE_depth...\n",
      "[18:05:50] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Model Evaluation Done\n",
      "0.00033576642992309364\n"
     ]
    }
   ],
   "source": [
    "repost_new=pd.read_csv(\"./data/trainRepost_new.csv\", nrows=500000,encoding=\"utf-8\")\n",
    "wb_list=list(set(context[\"wbID\"]).intersection(repost_new[\"wbID\"]))\n",
    "features=features3(wb_list,repost_new,context,features)\n",
    "train_data=y_train.merge(features,on=\"wbID\",how=\"left\")\n",
    "train_data=train_data[train_data[\"y_depth\"]!=0]\n",
    "\n",
    "#print(train_data)\n",
    "\n",
    "\n",
    "x_train3=train_data[['wbID','hour', 'y_width', 'y_depth',\n",
    "       'fans_number', 'width_15_min_y', 'depth_15_min_y', 'width_30_min_y',\n",
    "       'depth_30_min_y', 'width_45_min_y', 'depth_45_min_y', 'fans_sum_30',\n",
    "       'fans_mean_30', 'fans_max_30', 'fans_sum_15', 'fans_mean_15',\n",
    "      'fans_max_15', 'fans_sum_45', 'fans_mean_45', 'fans_max_45',\n",
    "       'fans_sum_60', 'fans_mean_60', 'fans_max_60', 'fans_mean_diff',\n",
    "       'fans_max_diff', 'fans_sum_diff', 'distance_0', 'topic_num', 'link_num',\n",
    "       'name_num', 'emoji_num',\"time\"]]\n",
    "y_train3=train_data[['wbID',\"y_width\",\"y_depth\",\"time\"]]\n",
    "x_train3[[\"depth_15_min\",\"depth_30_min\",\"depth_45_min\",\"depth\"]]=x_train3[[\"depth_15_min_y\",\"depth_30_min_y\",\"depth_45_min_y\",\"y_depth\"]].T.drop_duplicates().T.fillna(1)\n",
    "x_train3[['fans_sum_30', 'fans_mean_30', 'fans_max_30', 'fans_sum_15', 'fans_mean_15',\n",
    "      'fans_max_15', 'fans_sum_45', 'fans_mean_45', 'fans_max_45',\n",
    "       'fans_sum_60', 'fans_mean_60', 'fans_max_60', 'fans_mean_diff',\n",
    "       'fans_max_diff', 'fans_sum_diff',\"fans_number\"]]=x_train3[['fans_sum_30', 'fans_mean_30', 'fans_max_30', 'fans_sum_15', 'fans_mean_15',\n",
    "      'fans_max_15', 'fans_sum_45', 'fans_mean_45', 'fans_max_45',\n",
    "       'fans_sum_60', 'fans_mean_60', 'fans_max_60', 'fans_mean_diff',\n",
    "       'fans_max_diff', 'fans_sum_diff',\"fans_number\"]].fillna(x_train3[['fans_sum_30', 'fans_mean_30', 'fans_max_30', 'fans_sum_15', 'fans_mean_15',\n",
    "      'fans_max_15', 'fans_sum_45', 'fans_mean_45', 'fans_max_45',\n",
    "       'fans_sum_60', 'fans_mean_60', 'fans_max_60', 'fans_mean_diff',\n",
    "       'fans_max_diff', 'fans_sum_diff',\"fans_number\"]].mean())\n",
    "\n",
    "x_train3[\"time\"]=pd.to_numeric(x_train3[\"time\"])\n",
    "x_train3 = x_train3.fillna(0)\n",
    "y_train3=y_train3.fillna(1)\n",
    "\n",
    "rfr1= ensemble.RandomForestRegressor()\n",
    "rfr2= ensemble.RandomForestRegressor()\n",
    "x_train3 = x_train3.T.drop_duplicates().T\n",
    "mape_rf3,y_pred_width_rf3,y_width_rf3,y_pred_depth_rf3,y_depth_rf3=ModelEvaluation(rfr1,rfr2,x_train3, y_train3)\n",
    "print(mape_rf3)\n",
    "\n",
    "xgb1=XGBRegressor()\n",
    "xgb2=XGBRegressor()\n",
    "mape_xgb3,y_pred_width_xgb3,y_width3,y_pred_depth_xgb3,y_depth3=ModelEvaluation(xgb1,xgb2,x_train3, y_train3)\n",
    "print(mape_xgb3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_lgb3,y_pred_width_lgb3,y_te_width_lgb3,y_pred_depth_lgb3,y_te_depth_lgb3=ModelEvaluation(model_width,model_depth, x_train3, y_train3,parameters)\n",
    "print(\"score:\",mape_lgb3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_params = [{\"learning_rate\": [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1]}, \n",
    "#               {\"n_estimators\": [100, 300, 500, 1000]}, \n",
    "#               {\"max_depth\": range(3,10,2)}, \n",
    "#               {\"min_child_weight\": range(1,6,2)}, \n",
    "#               {\"gamma\": [i/10.0 for i in range(0,5)]}, \n",
    "#               {\"subsample\": [i/10.0 for i in range(6,10)]},\n",
    "#               {\"colsample_bytree\": [i/10.0 for i in range(6,10)]}, \n",
    "#               {\"reg_alpha\": [1e-5, 1e-2, 0.1, 1, 100]}]\n",
    "# xgb_best_params = ModelOptimization(XGBRegressor(), xgb_params, x_train.drop(\"wbID\",axis=1), y_train[\"y_width\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               wbID  hour  y_width  y_depth  fans_number  depth_15_min_y  \\\n",
      "0      3.953269e+15  12.0      1.0     75.0          1.0             0.0   \n",
      "1      3.953269e+15  12.0      1.0      1.0          1.0             0.0   \n",
      "2      3.970825e+15  23.0      1.0     75.0          1.0             0.0   \n",
      "4      3.918646e+15  23.0      1.0     75.0          1.0             0.0   \n",
      "6      3.967101e+15  16.0      6.0     75.0          1.0             1.0   \n",
      "7      3.967101e+15  16.0      6.0      1.0          1.0             1.0   \n",
      "8      3.921389e+15  13.0      1.0     75.0          1.0             1.0   \n",
      "9      3.921389e+15  13.0      1.0      1.0          1.0             1.0   \n",
      "10     3.968646e+15  22.0      7.0     75.0          1.0             1.0   \n",
      "11     3.968646e+15  22.0      7.0      1.0          1.0             1.0   \n",
      "12     3.923172e+15  11.0      1.0     75.0          1.0             0.0   \n",
      "14     3.972130e+15  13.0      1.0     75.0          1.0             0.0   \n",
      "15     3.972130e+15  13.0      1.0      1.0          1.0             0.0   \n",
      "16     3.896014e+15  12.0     11.0     75.0          1.0             1.0   \n",
      "17     3.896014e+15  12.0     11.0      1.0          1.0             1.0   \n",
      "18     3.976502e+15  15.0      5.0     75.0          1.0             1.0   \n",
      "19     3.976502e+15  15.0      5.0      1.0          1.0             1.0   \n",
      "20     3.862087e+15  21.0      1.0     75.0          1.0             0.0   \n",
      "21     3.862087e+15  21.0      1.0      1.0          1.0             0.0   \n",
      "22     3.800456e+15  20.0      4.0     75.0          1.0             0.0   \n",
      "23     3.800456e+15  20.0      4.0      1.0          1.0             0.0   \n",
      "24     3.969843e+15   6.0     15.0     75.0          1.0             1.0   \n",
      "25     3.969843e+15   6.0     15.0      1.0          1.0             1.0   \n",
      "26     3.930512e+15  17.0      1.0     75.0          1.0             0.0   \n",
      "28     3.935839e+15  10.0      2.0     75.0          1.0             1.0   \n",
      "29     3.935839e+15  10.0      2.0      1.0          1.0             1.0   \n",
      "30     3.971374e+15  11.0      2.0     75.0          1.0             1.0   \n",
      "31     3.971374e+15  11.0      2.0      1.0          1.0             1.0   \n",
      "32     3.904012e+15  14.0      4.0     75.0          1.0             1.0   \n",
      "33     3.904012e+15  14.0      4.0      1.0          1.0             1.0   \n",
      "...             ...   ...      ...      ...          ...             ...   \n",
      "51290  3.830854e+15  17.0      1.0     75.0          1.0             0.0   \n",
      "51292  3.947108e+15  12.0      8.0     75.0          1.0             1.0   \n",
      "51293  3.947108e+15  12.0      8.0      1.0          1.0             1.0   \n",
      "51294  3.939990e+15  21.0      6.0     75.0          1.0             0.0   \n",
      "51295  3.939990e+15  21.0      6.0      1.0          1.0             0.0   \n",
      "51296  3.968807e+15   9.0      1.0     75.0          1.0             0.0   \n",
      "51298  3.951769e+15   9.0      1.0     75.0          1.0             0.0   \n",
      "51299  3.951769e+15   9.0      1.0      1.0          1.0             0.0   \n",
      "51300  3.866942e+15   7.0      2.0     75.0          1.0             0.0   \n",
      "51301  3.866942e+15   7.0      2.0      1.0          1.0             0.0   \n",
      "51302  3.946881e+15  21.0      7.0     75.0          1.0             1.0   \n",
      "51303  3.946881e+15  21.0      7.0      1.0          1.0             1.0   \n",
      "51304  3.949274e+15  11.0      1.0     75.0          1.0             0.0   \n",
      "51306  3.963852e+15  17.0      2.0     75.0          1.0             0.0   \n",
      "51307  3.963852e+15  17.0      2.0      1.0          1.0             0.0   \n",
      "51308  3.945341e+15  15.0      6.0     75.0          1.0             1.0   \n",
      "51309  3.945341e+15  15.0      6.0      1.0          1.0             1.0   \n",
      "51310  3.807922e+15  10.0      1.0     75.0          1.0             0.0   \n",
      "51312  3.957760e+15  21.0     17.0     75.0          1.0             1.0   \n",
      "51313  3.957760e+15  21.0     17.0      1.0          1.0             1.0   \n",
      "51314  3.875801e+15  18.0      1.0     75.0          1.0             0.0   \n",
      "51316  3.838462e+15  17.0      7.0     75.0          1.0             1.0   \n",
      "51317  3.838462e+15  17.0      7.0      1.0          1.0             1.0   \n",
      "51318  3.975736e+15  12.0      7.0     75.0          1.0             1.0   \n",
      "51319  3.975736e+15  12.0      7.0      1.0          1.0             1.0   \n",
      "51320  3.925409e+15  15.0      1.0     75.0          1.0             1.0   \n",
      "51321  3.925409e+15  15.0      1.0      1.0          1.0             1.0   \n",
      "51322  3.916492e+15   0.0      1.0     75.0          1.0             0.0   \n",
      "51324  3.818979e+15  22.0      6.0     75.0          1.0             1.0   \n",
      "51325  3.818979e+15  22.0      6.0      1.0          1.0             1.0   \n",
      "\n",
      "       depth_30_min_y  depth_45_min_y  fans_mean_diff  distance_0  topic_num  \\\n",
      "0                 1.0             1.0             0.0  689.500000        1.0   \n",
      "1                 1.0             1.0             0.0  689.500000        1.0   \n",
      "2                 0.0             0.0             0.0   47.550000        0.0   \n",
      "4                 0.0             0.0             0.0   28.900000        0.0   \n",
      "6                 1.0             1.0             0.0  445.700000        2.0   \n",
      "7                 1.0             1.0             0.0  445.700000        2.0   \n",
      "8                 1.0             1.0             0.0  649.150000        1.0   \n",
      "9                 1.0             1.0             0.0  649.150000        1.0   \n",
      "10                1.0             1.0             0.0   64.950000        0.0   \n",
      "11                1.0             1.0             0.0   64.950000        0.0   \n",
      "12                0.0             0.0             0.0  675.033333        1.0   \n",
      "14                0.0             0.0             0.0  620.633333        0.0   \n",
      "15                0.0             0.0             0.0  620.633333        0.0   \n",
      "16                1.0             1.0             0.0  680.616667        0.0   \n",
      "17                1.0             1.0             0.0  680.616667        0.0   \n",
      "18                1.0             1.0             0.0  530.683333        0.0   \n",
      "19                1.0             1.0             0.0  530.683333        0.0   \n",
      "20                0.0             0.0             0.0  133.450000        1.0   \n",
      "21                0.0             0.0             0.0  133.450000        1.0   \n",
      "22                1.0             1.0             0.0  233.883333        0.0   \n",
      "23                1.0             1.0             0.0  233.883333        0.0   \n",
      "24                1.0             1.0             0.0  370.100000        1.0   \n",
      "25                1.0             1.0             0.0  370.100000        1.0   \n",
      "26                0.0             0.0             0.0  397.166667        1.0   \n",
      "28                1.0             1.0             0.0  611.083333        0.0   \n",
      "29                1.0             1.0             0.0  611.083333        0.0   \n",
      "30                1.0             1.0             0.0  693.983333        0.0   \n",
      "31                1.0             1.0             0.0  693.983333        0.0   \n",
      "32                1.0             1.0             0.0  579.500000        1.0   \n",
      "33                1.0             1.0             0.0  579.500000        1.0   \n",
      "...               ...             ...             ...         ...        ...   \n",
      "51290             0.0             0.0             0.0  400.950000        2.0   \n",
      "51292             1.0             1.0             0.0  689.950000        0.0   \n",
      "51293             1.0             1.0             0.0  689.950000        0.0   \n",
      "51294             1.0             1.0             0.0  174.066667        1.0   \n",
      "51295             1.0             1.0             0.0  174.066667        1.0   \n",
      "51296             0.0             0.0             0.0  572.916667        0.0   \n",
      "51298             0.0             0.0             0.0  549.566667        0.0   \n",
      "51299             0.0             0.0             0.0  549.566667        0.0   \n",
      "51300             0.0             1.0             0.0  438.083333        0.0   \n",
      "51301             0.0             1.0             0.0  438.083333        0.0   \n",
      "51302             1.0             1.0             0.0  152.316667        0.0   \n",
      "51303             1.0             1.0             0.0  152.316667        0.0   \n",
      "51304             0.0             0.0             0.0  716.083333        1.0   \n",
      "51306             0.0             0.0             0.0  394.750000        1.0   \n",
      "51307             0.0             0.0             0.0  394.750000        1.0   \n",
      "51308             1.0             1.0             0.0  513.883333        0.0   \n",
      "51309             1.0             1.0             0.0  513.883333        0.0   \n",
      "51310             0.0             0.0             0.0  633.533333        1.0   \n",
      "51312             1.0             1.0             0.0  122.233333        1.0   \n",
      "51313             1.0             1.0             0.0  122.233333        1.0   \n",
      "51314             0.0             0.0             0.0  359.600000        0.0   \n",
      "51316             1.0             1.0             0.0  411.950000        2.0   \n",
      "51317             1.0             1.0             0.0  411.950000        2.0   \n",
      "51318             1.0             1.0             0.0  694.166667        0.0   \n",
      "51319             1.0             1.0             0.0  694.166667        0.0   \n",
      "51320             1.0             1.0             0.0  513.816667        0.0   \n",
      "51321             1.0             1.0             0.0  513.816667        0.0   \n",
      "51322             0.0             0.0             0.0   52.083333        1.0   \n",
      "51324             1.0             1.0             0.0   69.500000        0.0   \n",
      "51325             1.0             1.0             0.0   69.500000        0.0   \n",
      "\n",
      "       link_num  name_num  emoji_num  \n",
      "0           0.0       2.0        1.0  \n",
      "1           0.0       2.0        1.0  \n",
      "2         200.0       0.0        0.0  \n",
      "4           1.0       0.0        0.0  \n",
      "6           1.0       0.0        3.0  \n",
      "7           1.0       0.0        3.0  \n",
      "8           0.0       0.0        1.0  \n",
      "9           0.0       0.0        1.0  \n",
      "10          0.0       0.0        0.0  \n",
      "11          0.0       0.0        0.0  \n",
      "12          2.0       0.0        0.0  \n",
      "14          0.0       0.0        0.0  \n",
      "15          0.0       0.0        0.0  \n",
      "16          0.0       0.0        0.0  \n",
      "17          0.0       0.0        0.0  \n",
      "18          1.0       1.0        0.0  \n",
      "19          1.0       1.0        0.0  \n",
      "20          0.0       0.0        0.0  \n",
      "21          0.0       0.0        0.0  \n",
      "22          0.0       0.0        3.0  \n",
      "23          0.0       0.0        3.0  \n",
      "24          0.0       0.0        0.0  \n",
      "25          0.0       0.0        0.0  \n",
      "26          0.0       0.0        0.0  \n",
      "28          0.0       0.0        1.0  \n",
      "29          0.0       0.0        1.0  \n",
      "30          0.0       0.0        0.0  \n",
      "31          0.0       0.0        0.0  \n",
      "32          0.0       0.0        3.0  \n",
      "33          0.0       0.0        3.0  \n",
      "...         ...       ...        ...  \n",
      "51290       0.0       0.0        6.0  \n",
      "51292       1.0       0.0        1.0  \n",
      "51293       1.0       0.0        1.0  \n",
      "51294       0.0       0.0        0.0  \n",
      "51295       0.0       0.0        0.0  \n",
      "51296       1.0       0.0        0.0  \n",
      "51298       0.0       0.0        0.0  \n",
      "51299       0.0       0.0        0.0  \n",
      "51300       0.0       0.0        0.0  \n",
      "51301       0.0       0.0        0.0  \n",
      "51302       1.0       0.0        0.0  \n",
      "51303       1.0       0.0        0.0  \n",
      "51304       2.0       0.0        0.0  \n",
      "51306       0.0       0.0        1.0  \n",
      "51307       0.0       0.0        1.0  \n",
      "51308       1.0       0.0        1.0  \n",
      "51309       1.0       0.0        1.0  \n",
      "51310       0.0       0.0        0.0  \n",
      "51312       0.0       1.0        0.0  \n",
      "51313       0.0       1.0        0.0  \n",
      "51314       1.0       0.0        0.0  \n",
      "51316       1.0       0.0        1.0  \n",
      "51317       1.0       0.0        1.0  \n",
      "51318       0.0       1.0        0.0  \n",
      "51319       0.0       1.0        0.0  \n",
      "51320       1.0       0.0        0.0  \n",
      "51321       1.0       0.0        0.0  \n",
      "51322       0.0       1.0        1.0  \n",
      "51324       0.0       0.0        0.0  \n",
      "51325       0.0       0.0        0.0  \n",
      "\n",
      "[44278 rows x 14 columns]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
